{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model(model):\n",
    "    \"\"\"\n",
    "    Clear a tensorflow model from memory & garbage collector.\n",
    "    :param model: Tensorflow model to remove.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Memory handling\n",
    "    del model  # Manually delete model\n",
    "    tf.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_model():\n",
    "    return [random.randint(lb[0], ub[0]),  # batch_size\n",
    "             random.randint(lb[1], ub[1]), random.randint(lb[2], ub[2]),  # epoch_size, optimizer\n",
    "             random.randint(lb[3], ub[3]), random.randint(lb[4], ub[4]), random.randint(lb[5], ub[5]),  # units\n",
    "             random.uniform(lb[6], ub[6]), random.uniform(lb[7], ub[7]), random.uniform(lb[8], ub[8]),  # dropout\n",
    "             random.uniform(lb[9], ub[9]), random.uniform(lb[10], ub[10]), random.uniform(lb[11], ub[11]),  # recurrent_dropout\n",
    "             random.uniform(lb[12], ub[12]), random.uniform(lb[13], ub[13]), random.uniform(lb[14], ub[14]),  # gaussian noise std\n",
    "             random.randint(lb[15], ub[15]), random.randint(lb[16], ub[16]), random.randint(lb[17], ub[17]), # gaussian_noise\n",
    "             random.randint(lb[18], ub[18]), random.randint(lb[19], ub[19]), random.randint(lb[20], ub[20])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Search Space bounds\n",
    "# TODO: Add weights initializer search: https://keras.io/initializers/\n",
    "bounds = [(7, 1 * 31),  # batch_size (~ #days: week, month, year)  # TODO: reduced batch size to try avoiding OOM\n",
    "          (350, 600), (0, 4),  # , 5)    # epoch_size, optimizer\n",
    "          # (1023, 1024), (1023, 1024), (1023, 1024),  # units\n",
    "          (64, 512), (64, 512), (64, 512),\n",
    "          # (32, 512), (32, 196), (32, 384),\n",
    "          (0.01, 0.25), (0.01, 0.25), (0.01, 0.25),  # dropout\n",
    "          (0.01, 0.25), (0.01, 0.25), (0.01, 0.25),  # recurrent_dropout\n",
    "          (0.01, 1), (0.01, 1), (0.01, 1),  # gaussian noise std\n",
    "          (0, 1), (0, 1), (0, 1),  # gaussian_noise\n",
    "          (0, 1), (0, 1), (0, 1)]  # batch normalization\n",
    "\n",
    "# Lower Bounds\n",
    "lb = [bounds[0][0],  # batch_size\n",
    "      bounds[1][0], bounds[2][0],  # epoch_size, optimizer\n",
    "      bounds[3][0], bounds[4][0], bounds[5][0],  # units\n",
    "      bounds[6][0], bounds[7][0], bounds[8][0],  # dropout\n",
    "      bounds[9][0], bounds[10][0], bounds[11][0],  # recurrent_dropout\n",
    "      bounds[12][0], bounds[13][0], bounds[14][0],  # gaussian noise std\n",
    "      bounds[15][0], bounds[16][0], bounds[17][0],  # gaussian_noise\n",
    "      bounds[18][0], bounds[19][0], bounds[20][0]]  # batch normalization\n",
    "\n",
    "# Upper Bounds\n",
    "ub = [bounds[0][1],  # batch_size\n",
    "      bounds[1][1], bounds[2][1],  # epoch_size, optimizer\n",
    "      bounds[3][1], bounds[4][1], bounds[5][1],  # units\n",
    "      bounds[6][1], bounds[7][1], bounds[8][1],  # dropout\n",
    "      bounds[9][1], bounds[10][1], bounds[11][1],  # recurrent_dropout\n",
    "      bounds[12][1], bounds[13][1], bounds[14][1],  # gaussian noise std\n",
    "      bounds[15][1], bounds[16][1], bounds[17][1],  # gaussian_noise\n",
    "      bounds[18][1], bounds[19][1], bounds[20][1]]  # batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, *args):\n",
    "#     train_model.counter += 1\n",
    "#     modelLabel = train_model.label\n",
    "#     modelFolds = train_model.folds\n",
    "#     data_manipulation = train_model.data_manipulation\n",
    "#     rank = data_manipulation[\"rank\"]\n",
    "#     master = data_manipulation[\"master\"]\n",
    "#     directory = data_manipulation[\"directory\"]\n",
    "#     filePrefix = data_manipulation[\"filePrefix\"]\n",
    "#     island = data_manipulation[\"island\"]\n",
    "#     verbosity = data_manipulation[\"verbose\"]\n",
    "#     multi_gpu = data_manipulation[\"multi_gpu\"]\n",
    "#     store_plots = data_manipulation[\"storePlots\"]\n",
    "\n",
    "#     x_data, y_data = args\n",
    "\n",
    "    # if island == \"bh\" or island == \"sg\":  # TODO: un-normalize data\n",
    "    #     print(\"bounds \", data_manipulation[\"bounds\"])\n",
    "    #     print(\"x \", x)\n",
    "    #     for i in range(len(x)):\n",
    "    #         x[i] = x[i] * (data_manipulation[\"bounds\"][i][1] - data_manipulation[\"bounds\"][i][0]) \\\n",
    "    #                + data_manipulation[\"bounds\"][i][0]\n",
    "    #     x = np.array(x)\n",
    "    #     print(\"un-normalized x \", x)\n",
    "\n",
    "    # x = [32.269684115953126, 478.4579158867764, 2.4914987273745344, 291.55476719406147, 32.0, 512.0, 0.0812481431483004,\n",
    "    #      0.01, 0.1445004524623349, 0.22335740221774894, 0.03443050512961357, 0.05488258021289669, 1.0,\n",
    "    #      0.620275664519184, 0.34191582396595566, 0.9436131979280933, 0.4991752935129543, 0.4678261851228459, 0.0,\n",
    "    #      0.355287972380982, 0.0]  # TODO: Temp set the same model to benchmark a specific DNN\n",
    "\n",
    "    full_model_parameters = np.array(x.copy())\n",
    "#     if data_manipulation[\"fp16\"]:\n",
    "#         full_model_parameters.astype(np.float32, casting='unsafe')  # TODO: temp test speed of keras with fp16\n",
    "\n",
    "#     print(\"\\n=============\\n\")\n",
    "#     print(\"--- Rank {}: {} iteration {} using: {}\".format(rank, modelLabel, train_model.counter, x[6:15]))\n",
    "\n",
    "    dropout1 = x[6]\n",
    "    dropout2 = x[7]\n",
    "    dropout3 = x[8]\n",
    "    recurrent_dropout1 = x[9]\n",
    "    recurrent_dropout2 = x[10]\n",
    "    recurrent_dropout3 = x[11]\n",
    "\n",
    "    # Gaussian noise\n",
    "    noise_stddev1 = x[12]\n",
    "    noise_stddev2 = x[13]\n",
    "    noise_stddev3 = x[14]\n",
    "\n",
    "    x = np.rint(x).astype(np.int32)\n",
    "    optimizers = ['adadelta', 'adagrad', 'nadam', 'adamax',\n",
    "                  'adam', 'amsgrad']  # , 'rmsprop', 'sgd'] # Avoid loss NaNs, by removing rmsprop & sgd\n",
    "    batch_size = x[0]\n",
    "    epoch_size = x[1]\n",
    "    optimizer = optimizers[x[2]]\n",
    "    units1 = x[3]\n",
    "    units2 = x[4]\n",
    "    units3 = x[5]\n",
    "\n",
    "    # Batch normalization\n",
    "    use_batch_normalization1 = x[15]\n",
    "    use_batch_normalization2 = x[16]\n",
    "    use_batch_normalization3 = x[17]\n",
    "    use_gaussian_noise1 = x[18]\n",
    "    use_gaussian_noise2 = x[19]\n",
    "    use_gaussian_noise3 = x[20]\n",
    "\n",
    "#     print(\"--- Rank {}: batch_size: {}, epoch_size: {} Optimizer: {}, LSTM Unit sizes: {} \"\n",
    "#           \"Batch Normalization/Gaussian Noise: {}\"\n",
    "#           .format(rank, x[0], x[1], optimizers[x[2]], x[3:6], x[15:21]))\n",
    "\n",
    "#     x_data, x_data_holdout = x_data[:-365], x_data[-365:]\n",
    "#     y_data, y_data_holdout = y_data[:-365], y_data[-365:]\n",
    "\n",
    "#     totalFolds = modelFolds\n",
    "#     timeSeriesCrossValidation = TimeSeriesSplit(n_splits=totalFolds)\n",
    "    # timeSeriesCrossValidation = KFold(n_splits=totalFolds)\n",
    "\n",
    "    smape_scores = []\n",
    "    mse_scores = []\n",
    "    train_mse_scores = []\n",
    "    # dev_mse_scores = []\n",
    "    current_fold = 0\n",
    "\n",
    "    # TODO: (Baldwin) phenotypic plasticity, using random uniform.\n",
    "    min_regularizer = 0.0\n",
    "    max_regularizer = 0.01\n",
    "    regularizer_chance = 0.1\n",
    "    regularizer_chance_randoms = np.random.rand(9)\n",
    "    core_layers_randoms = np.random.randint(4, size=5)  # TODO: Dense, LSTM, BiLSTM, GRU, BiGRU\n",
    "\n",
    "    l1_l2_randoms = np.random.uniform(low=min_regularizer, high=max_regularizer, size=(9, 2))\n",
    "    \n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    lstm_kwargs = {'units': units1, 'dropout': dropout1, 'recurrent_dropout': recurrent_dropout1,\n",
    "                   'return_sequences': True,\n",
    "                   'implementation': 2,\n",
    "                   # 'kernel_regularizer': l2(0.01),\n",
    "                   # 'activity_regularizer': l2(0.01),\n",
    "                   # 'bias_regularizer': l2(0.01)    # TODO: test with kernel, activity, bias regularizers\n",
    "                   }\n",
    "    # Local mutation\n",
    "    if regularizer_chance_randoms[0] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[0, 0], l1_l2_randoms[0, 1])\n",
    "    if regularizer_chance_randoms[1] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[1, 0], l1_l2_randoms[2, 1])\n",
    "    if regularizer_chance_randoms[2] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[2, 0], l1_l2_randoms[0, 1])\n",
    "\n",
    "    # 1st base layer\n",
    "    # model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs), input_shape=(x_data.shape[1], x_data.shape[2])))  # input_shape: rows: n, timestep: 1, features: m\n",
    "    if core_layers_randoms[0] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_randoms[0] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_randoms[0] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_randoms[0] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "        # model.add(tf.keras.layers.Dense(units3,\n",
    "        #                                 activity_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[3, 0],\n",
    "        #                                                                                  l1_l2_randoms[3, 1]),\n",
    "        #                                 bias_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[4, 0],\n",
    "        #                                                                              l1_l2_randoms[4, 1]),\n",
    "        #                                 kernel_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[5, 0],\n",
    "        #                                                                                l1_l2_randoms[5, 1])))\n",
    "\n",
    "    # 2nd base layer\n",
    "    if use_gaussian_noise1 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev1))\n",
    "    if use_batch_normalization1 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    lstm_kwargs['units'] = units2\n",
    "    lstm_kwargs['dropout'] = dropout2\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout2\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[3] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[3, 0], l1_l2_randoms[3, 1])\n",
    "    if regularizer_chance_randoms[4] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[4, 0], l1_l2_randoms[4, 1])\n",
    "    if regularizer_chance_randoms[5] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[5, 0], l1_l2_randoms[5, 1])\n",
    "    # 2nd base layer\n",
    "    if core_layers_randoms[1] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_randoms[1] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_randoms[1] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_randoms[1] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "        # model.add(tf.keras.layers.Dense(units3,\n",
    "        #                                 activity_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[3, 0],\n",
    "        #                                                                                  l1_l2_randoms[3, 1]),\n",
    "        #                                 bias_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[4, 0],\n",
    "        #                                                                              l1_l2_randoms[4, 1]),\n",
    "        #                                 kernel_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[5, 0],\n",
    "        #                                                                                l1_l2_randoms[5, 1])))\n",
    "\n",
    "    if use_gaussian_noise2 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev2))\n",
    "    if use_batch_normalization2 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd base layer\n",
    "    lstm_kwargs['units'] = units3\n",
    "    lstm_kwargs['dropout'] = dropout3\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout3\n",
    "    lstm_kwargs['return_sequences'] = False  # Last layer should return sequences\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[6] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[6, 0], l1_l2_randoms[6, 1])\n",
    "    if regularizer_chance_randoms[7] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[7, 0], l1_l2_randoms[7, 1])\n",
    "    if regularizer_chance_randoms[8] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[8, 0], l1_l2_randoms[8, 1])\n",
    "    # model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    if core_layers_randoms[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_randoms[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_randoms[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_randoms[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "        # model.add(tf.keras.layers.Dense(units3,\n",
    "        #                                 activity_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[3, 0],\n",
    "        #                                                                                  l1_l2_randoms[3, 1]),\n",
    "        #                                 bias_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[4, 0],\n",
    "        #                                                                              l1_l2_randoms[4, 1]),\n",
    "        #                                 kernel_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[5, 0],\n",
    "        #                                                                                l1_l2_randoms[5, 1])))\n",
    "    if use_gaussian_noise3 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev3))\n",
    "    if use_batch_normalization3 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # model.add(tf.keras.layers.Dense(y_data.shape[1], activation=random.choice(\n",
    "    #     [\"tanh\", \"softmax\", \"elu\", \"selu\", \"softplus\", \"relu\", \"softsign\", \"hard_sigmoid\",\n",
    "    #      \"linear\"])))  # TODO: test with 2 extra dense layers\n",
    "#     model.add(tf.keras.layers.Dense(y_data.shape[1]))  # TODO: shape\n",
    "    model.add(tf.keras.layers.Dense(16))\n",
    "#     if multi_gpu:\n",
    "#         model = tf.keras.utils.multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    if optimizer == 'amsgrad':  # Adam variant: amsgrad (boolean), \"On the Convergence of Adam and Beyond\".\n",
    "        model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(amsgrad=True))\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    current_fold += 1  # TODO: train, trainValidation, validation\n",
    "#     print(\"--- Rank {}: Current Fold: {}/{}\".format(rank, current_fold, totalFolds))\n",
    "\n",
    "    early_stop = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto'),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='auto',\n",
    "                                             cooldown=1, verbose=1),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ]\n",
    "\n",
    "#     try:\n",
    "#         history = model.fit(x_data[train], y_data[train],\n",
    "#                             verbose=verbosity,\n",
    "#                             batch_size=batch_size,\n",
    "#                             epochs=epoch_size,\n",
    "#                             validation_data=(x_data[validation], y_data[validation]),\n",
    "#                             callbacks=early_stop)\n",
    "#     except ValueError:\n",
    "# #         print(\"--- Rank {}: Value Error exception: Model fit exception. Trying again...\".format(rank))\n",
    "#         history = model.fit(x_data[train], y_data[train],\n",
    "#                             verbose=verbosity,\n",
    "#                             batch_size=batch_size,\n",
    "#                             epochs=epoch_size,\n",
    "#                             validation_data=(x_data[validation], y_data[validation]),\n",
    "#                             callbacks=early_stop)\n",
    "#     except:\n",
    "# #         print(\"--- Rank {}: Exception: Returning max float value for this iteration.\".format(rank))\n",
    "#         print(\"--- Exception: Returning max float value for this iteration.\")\n",
    "#         delete_model(model)\n",
    "\n",
    "#         return sys.float_info.max\n",
    "\n",
    "    print('.', end='')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\temp3rr0r\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "for k in range(5000):\n",
    "    x = get_random_model()\n",
    "    args = {}\n",
    "    args[\"modelLabel\"] = \"test\"\n",
    "    train_model.counter = 1\n",
    "    train_model.label = \"test\"\n",
    "    train_model.folds = 1\n",
    "    train_model.data_manipulation = args\n",
    "    result = train_model(x, *args)\n",
    "    if result != 0:\n",
    "        print(\"Invalid model\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
